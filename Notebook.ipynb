{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import used libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing :\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# Data Visualisation:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Model Building, Training, and Testing:\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Neural Networks :\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM, BatchNormalization\n",
    "from keras.layers import Dropout, Conv1D, MaxPooling1D, Input, GRU\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Utils:\n",
    "from IPython.display import display, HTML\n",
    "# from collections import deque  \n",
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model:\n",
    "The next function is used to build a GRU neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_Model(GRU_layers, dense_layers, input_shape):\n",
    "    # GRU Neural Network:\n",
    "    GRU_Model = Sequential()\n",
    "    # Input layer:\n",
    "    GRU_Model.add(GRU(GRU_layers[0], activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    # GRU Layers: \n",
    "    if len(GRU_layers)>1:\n",
    "        for i, n_nodes in enumerate(GRU_layers[1:]):\n",
    "            if len(GRU_layers[1:]) - i >= 2 :\n",
    "                GRU_Model.add(GRU(n_nodes, activation='relu', return_sequences=True))\n",
    "            else: \n",
    "                GRU_Model.add(GRU(n_nodes, activation='relu'))\n",
    "    \n",
    "    # Dense Layers:\n",
    "    for d in dense_layers:\n",
    "        GRU_Model.add(Dense(d, activation='relu'))\n",
    "    \n",
    "    # Output Layer :\n",
    "    GRU_Model.add(Dense(1))\n",
    "    GRU_Model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    return GRU_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model:\n",
    "The next function is used to build an LSTM neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(LSTM_layers, dense_layers, input_shape):\n",
    "    # LSTM Neural Network:\n",
    "    LSTM_Model = Sequential()\n",
    "    # Input layer:\n",
    "    LSTM_Model.add(LSTM(LSTM_layers[0], activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    # LSTM Layers: \n",
    "    if len(LSTM_layers)>1:\n",
    "        for i, n_nodes in enumerate(LSTM_layers[1:]):\n",
    "            if len(LSTM_layers[1:]) - i >= 2 :\n",
    "                LSTM_Model.add(LSTM(n_nodes, activation='relu', return_sequences=True))\n",
    "            else: \n",
    "                LSTM_Model.add(LSTM(n_nodes, activation='relu'))\n",
    "    \n",
    "    # Dense Layers:\n",
    "    for d in dense_layers:\n",
    "        LSTM_Model.add(Dense(d, activation='relu'))\n",
    "    \n",
    "    # Output Layer :\n",
    "    LSTM_Model.add(Dense(1))\n",
    "    LSTM_Model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    return LSTM_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model:\n",
    "The next function is used to build a convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inp, filters=64, bn=True, kernel_size=2,\n",
    "               pool=True, dropout = 0.2):\n",
    "    _ = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(inp)\n",
    "    if bn:\n",
    "        _ = BatchNormalization()(_)\n",
    "    if pool:\n",
    "        _ = MaxPooling1D(pool_size=2)(_)\n",
    "    if dropout > 0:\n",
    "        _ = Dropout(0.2)(_)\n",
    "    return _\n",
    "\n",
    "def CNN_Model(input_shape, conv_layers, dense_layers):\n",
    "    input_layer = Input(shape = input_shape)\n",
    "    _ = conv_block(input_layer, filters=conv_layers[0], bn=False, pool=False)\n",
    "    if len(conv_layers)>1:\n",
    "        for c in conv_layers[1:]:\n",
    "            _ = conv_block(_, filters=c)\n",
    "\n",
    "    _  = Flatten()(_)\n",
    "\n",
    "    # for Action1 calculation\n",
    "    for d in dense_layers:\n",
    "        _ = Dense(units=d, activation='relu')(_)\n",
    "\n",
    "    output = Dense(units=1, name='output')(_)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[output])\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                  loss={'output': 'mae'},\n",
    "                  metrics={'output': 'mae'})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Class:\n",
    "The next class is used to hold a data batch and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "\n",
    "    def __init__(self, name, df = None, data_path = None, test_size = 0.25, n_steps = 4, verbose = False):\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.test_size = test_size\n",
    "        self.n_steps   = n_steps\n",
    "        self.verbose = verbose\n",
    "        if data_path and not df:\n",
    "            self.load_data()\n",
    "    \n",
    "    \n",
    "    def darw_graph(self, df, y_list, x_label ,y_label, n_shape, name, reshape  = False, w=15, h=5):\n",
    "        if(reshape):\n",
    "            plt.figure(figsize=(w, h))\n",
    "        ax = plt.gca()\n",
    "        for y in y_list:\n",
    "            df.plot(kind='line',y=y,ax=ax)\n",
    "        plt.xlabel(f\"{x_label}\\n\\nFigure({n_shape})\\n{name}\")\n",
    "        plt.ylabel(y_label)\n",
    "        plt.show()\n",
    "        if n_shape : n_shape += 1\n",
    "        else : n_shape = 1\n",
    "        return n_shape\n",
    "\n",
    "    # split a univariate sequence into samples:\n",
    "    def split_sequence(self, sequence, n_steps):\n",
    "        X, y = list(), list()\n",
    "        for i in range(len(sequence)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + n_steps\n",
    "            # check if we are beyond the sequence\n",
    "            if end_ix > len(sequence)-1:\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return array(X), array(y)\n",
    "\n",
    "\n",
    "    # Process the Data :\n",
    "    def process_data(self, data_type = \"all\"):\n",
    "        # Merge the data of the two tanks:\n",
    "        self.df[\"Tank level\"] = self.df[\"T1\"] + self.df[\"T2\"]\n",
    "        # Convert Date Time column from string to DateTime object :\n",
    "        self.df[\"Date Time\"] = self.df[\"Date Time\"].apply(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M'))\n",
    "        # Create columns for (Year, Month, WeekDay, Day, DayQ, Hour ..etc):\n",
    "        self.df['year'] = self.df['Date Time'].apply(lambda x: x.year)\n",
    "        self.df['month'] = self.df['Date Time'].apply(lambda x: x.month)\n",
    "        self.df['day'] = self.df['Date Time'].apply(lambda x: x.day)\n",
    "\n",
    "        self.df['weekday'] = self.df['Date Time'].apply(lambda x: x.weekday()) # Mon=0, Sun=1, .. Thur=5, Fri = 6\n",
    "        self.df['weekend'] = self.df['weekday'].apply(lambda x: 1 if x in [5,6] else 0) # = 1 if the day is a weekend day (Thursday, Friday)\n",
    "\n",
    "\n",
    "        self.df['hour'] = self.df['Date Time'].apply(lambda x: x.hour)\n",
    "        self.df['dayQ'] = self.df['hour'].apply(lambda x: x//6 +1)\n",
    "        \n",
    "        # Create new features (Refilled, Refilled Amount, Consumption):\n",
    "        self.df[\"refilled\"] = 0\n",
    "        self.df[\"refilled_Amount\"] = 0.0\n",
    "        self.df[\"consumption\"] = 0.0\n",
    "\n",
    "        for i in range(1,len(self.df[\"refilled\"]),1):\n",
    "            # Compute consumption in leters:\n",
    "            self.df[\"consumption\"][i] = self.df[\"Tank level\"][i-1] - self.df[\"Tank level\"][i] \n",
    "            # Correct some outliers ( 0 > x >= -5  --> x=0):\n",
    "            if((self.df[\"consumption\"][i] < 0) and (self.df[\"consumption\"][i] >= -5)): self.df[\"consumption\"][i] = 0\n",
    "\n",
    "            # Knowing what readings have anomalies or readings where the tank was filled before geting the reading :\n",
    "            if(self.df[\"Tank level\"][i] - self.df[\"Tank level\"][i-1] > 5 ):\n",
    "                self.df[\"refilled\"][i]        =  1\n",
    "                self.df[\"refilled_Amount\"][i] =  self.df[\"Tank level\"][i] - self.df[\"Tank level\"][i-1]\n",
    "\n",
    "        # Get some information about outliers :\n",
    "        i1 = self.df.loc[ ((self.df[\"refilled_Amount\"] < 20) & (self.df[\"refilled_Amount\"] > 0))].shape[0]\n",
    "        i2 = self.df.loc[ self.df[\"refilled_Amount\"] >= 20].shape[0]\n",
    "        i3 = self.df.loc[ self.df[\"consumption\"] < 0].shape[0]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Number of samples with reading errors : {i1} \")\n",
    "            print(f\"Number of samples that were taken after refilling the tank : {i2} \")\n",
    "            print(f\"Total number of outliers : {i3} \")\n",
    "        \n",
    "        # Delete outliers and reset the dataset without the outliers :\n",
    "        if self.verbose:\n",
    "            print(f\"Number of samples before deleting the outliers : {self.df.shape[0]}\")\n",
    "            '''Include only non-outlier samples (Exclude outliers) :\n",
    "               In our data, an outlier is a reading where the consumption value is negative,\n",
    "               so we will only include samples that have consumption value greater or equal to zero.'''\n",
    "        self.df = self.df.loc[ self.df[\"consumption\"] >= 0].reset_index(drop=True)\n",
    "        if self.verbose: print(f\"Number of samples after deleting the outliers : {self.df.shape[0]}\")\n",
    "        \n",
    "        # Shift Consumption column one step to make it predict the future consumption :\n",
    "        self.df['consumption'] = self.df['consumption'].shift(-1)\n",
    "        self.df.dropna(inplace = True)\n",
    "\n",
    "        # Select Columns of Concern:\n",
    "        data_columns = [\"month\", \"weekday\", \"weekend\", \"day\", \"dayQ\", \"hour\"]\n",
    "        target_column = [\"consumption\"]\n",
    "\n",
    "        if data_type == \"normal\" or data_type == \"all\" :\n",
    "            # Splitting Dataset to Training data and Test data:\n",
    "            self.train_X, self.test_X, self.train_y, self.test_y = train_test_split(self.df[data_columns].values,\n",
    "                                                  self.df[target_column].values,\n",
    "                                                  test_size = self.test_size , random_state = 14,\n",
    "                                                  shuffle = True)\n",
    "        elif data_type == \"time_series\" or data_type == \"all\":\n",
    "            # define input sequence\n",
    "            raw_seq = self.df[\"consumption\"].values\n",
    "            # choose a number of time steps\n",
    "            n_steps = self.n_steps\n",
    "            # split into samples\n",
    "            X, y = self.split_sequence(raw_seq, n_steps)\n",
    "            # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "            n_features = 1\n",
    "            X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "            # Splitting Dataset to Training data and Test data:\n",
    "            self.ts_train_X, self.ts_test_X, self.ts_train_y, self.ts_test_y = train_test_split(X, y,\n",
    "                                                                    test_size = self.test_size ,\n",
    "                                                                    random_state = 14, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_data(self):\n",
    "        # Visualizing Tank level and Diesel consumption over time:\n",
    "        # Tank level and consumption values over time before cleaning the data:\n",
    "        n_shape = self.darw_graph(df=self.df, y_list = ['consumption', 'Tank level'],\n",
    "                    x_label = \"Time\" ,y_label = \"Consumption - Tank Level\",\n",
    "                    n_shape = n_shape,\n",
    "                    name = \"Tank level and consumption values over time before cleaning the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch Class:\n",
    "The next class is used to perform a grid search on several models and choose the best of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "    def __init__(self, model_name, kwargs, train_X, train_y, test_X, test_y,\n",
    "                 n_best_models = 3, metric = \"mean_absolute_error\", PATH = \"\",\n",
    "                 ITS = None):\n",
    "        # model_name has to be the name used to call the model from its package\n",
    "        self.model_name = model_name\n",
    "        # Number of best models to choose:\n",
    "        self.n_best_models = n_best_models\n",
    "        # A list that holds models sorted by their performance\n",
    "        self.models_sorted_list = [] \n",
    "        # Should have the same names of given model's arguments, \n",
    "        self.kwargs = kwargs\n",
    "        self.results = pd.DataFrame(columns = list(self.kwargs.keys()))\n",
    "        # metric has to be the name used to call the metric function from its package\n",
    "        # Ex : to use MAE from scikit-learn metric should be \"mean_absolute_error\"\n",
    "        self.metric = metric\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        # ITS : Is Time Series (a dictionary that has some used values if data is time series data)\n",
    "        self.ITS = ITS \n",
    "        self.model_info = {}\n",
    "        self.PATH = PATH\n",
    "    \n",
    "    def make_models(self):\n",
    "        '''Generates the models to be tested'''\n",
    "        # This list stores the generated models\n",
    "        models_list = []\n",
    "        # \n",
    "        grid_search_string = \"\"\n",
    "        j = 0\n",
    "        # Generate the for loops :\n",
    "        for i,(k,v) in enumerate(self.kwargs.items()):\n",
    "            j=i+1\n",
    "            grid_search_string += \" \"*i*4 + f\"for arg{i} in self.kwargs['{k}']:\\n\"\n",
    "        temp = \"\".join([f\"{k} = arg{i}, \" for i,(k,_) in enumerate(self.kwargs.items())])\n",
    "        grid_search_string += \" \"*j*4 + f'models_list.append(eval(\"{self.model_name}({temp})\"))'+\"\\n\"\n",
    "        temp = \"\".join([\"self.model_info[models_list[-1]]={\"]+[f\"'{k}' : arg{i}, \" for i,(k,_) in enumerate(self.kwargs.items())]+[\"}\"])\n",
    "        grid_search_string += \" \"*j*4 + f'exec(\"{temp}\")'\n",
    "        print(grid_search_string)\n",
    "        exec(grid_search_string)\n",
    "        self.models_list = models_list\n",
    "\n",
    "    def search(self):\n",
    "        for i,model in tqdm(enumerate(self.models_list),desc= \"Training \", unit = \"Model\"):\n",
    "            start = time.time()\n",
    "            if not self.ITS : model.fit(self.train_X, self.train_y)\n",
    "            else : model.fit(self.train_X, self.train_y, epochs = self.ITS[\"epochs\"], batch_size = self.ITS[\"batch_size\"],\n",
    "                            verbose = 0, validation_split = 0.3, callbacks=self.ITS[\"callbacks\"]) \n",
    "            end = time.time()\n",
    "            training_time = round(end-start,3)\n",
    "            # print(training_time)\n",
    "\n",
    "            # Use Trained model to predict test data :\n",
    "            model_Pred    = model.predict(self.test_X)\n",
    "            if self.ITS : model_Pred    = model.predict(self.test_X).flatten()\n",
    "\n",
    "            # Calculate Goodness:\n",
    "            exec(f\"{self.metric}_res = {self.metric}(self.test_y, model_Pred)\")\n",
    "            self.models_sorted_list.append((model, eval(f\"{self.metric}_res\")))\n",
    "            # print(eval(f\"{self.metric}_res\"))\n",
    "            # Update Results DataFrames:\n",
    "            temp = \"\"\n",
    "            for k,_ in self.kwargs.items():\n",
    "                k1 = k\n",
    "                t1 = eval(f\"self.model_info[model]['{k1}']\")\n",
    "                if str(t1).isnumeric():temp += f\"'{k}' : {t1}, \"\n",
    "                else: temp += f\"'{k}' : '{t1}', \"\n",
    "            temp += f\" 'Training Time' : {training_time}, '{self.metric}_res': {eval(f'{self.metric}_res')}\" \n",
    "            \n",
    "            \n",
    "            # print(temp)\n",
    "            exec(\"self.results = self.results.append({\"+temp+\"}, ignore_index=True)\")\n",
    "            self.results = self.results.sort_values(by = f\"{self.metric}_res\")\n",
    "            self.results.to_csv(f\"{self.PATH}{self.model_name}_Results.csv\")\n",
    "        # Sort the list ascendingly (from min to max):\n",
    "        self.models_sorted_list = [(model, res) for model, res in sorted(self.models_sorted_list, key=lambda item: item[1])]\n",
    "        self.models_sorted_list = self.models_sorted_list[:self.n_best_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeightedAverage Class:\n",
    "The next class is used to make a fusion of several models and find the best combinatioon of models that would make the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedAverage:\n",
    "    def __init__(self, models_dic, is_time_series):\n",
    "        # models_dic : A list of dictionaries that holds models and info about them\n",
    "        # models_dic = [{\"name\":\"Model1\", \"model\":modelObject},{\"name\":\"Model2\", \"model\":modelObject2}, ... ]\n",
    "        self.models_dic = models_dic\n",
    "        self.is_time_series = is_time_series\n",
    "        \n",
    "    def calculate_metric(self, test_X, test_y):\n",
    "        for model in self.models_dic:\n",
    "            if self.is_time_series : prediction = model[\"model\"].predict(test_X).flatten()\n",
    "            else : prediction = model[\"model\"].predict(test_X)\n",
    "            mae = mean_absolute_error(test_y, prediction)\n",
    "            model['MAE'] = mae\n",
    "\n",
    "\n",
    "    def get_weights(self, comb):\n",
    "        total_mae = sum([self.models_dic[model_idx][\"MAE\"] for model_idx in comb])\n",
    "        weights = {}\n",
    "        for model_idx in comb:\n",
    "            weights[model_idx]= self.models_dic[model_idx][\"MAE\"]/total_mae\n",
    "        return weights\n",
    "\n",
    "    def find_best_combination(self, data_X, data_y):\n",
    "        # Generate the combinations :\n",
    "        self.combs = []\n",
    "        for n in range(1,len(self.models_dic)+1,1):\n",
    "            self.combs += list(itertools.combinations(range(len(self.models_dic)), n))\n",
    "            \n",
    "        self.combs_res = self.predict(data_X = data_X, data_y = data_y, return_metric=True)\n",
    "        \n",
    "        # Get the best combination depending on the best result:\n",
    "        self.best_combination = self.combs[np.argmin(self.combs_res)]\n",
    "        self.best_models_list = [self.models_dic[i]['model'] for i in self.best_combination]\n",
    "    \n",
    "    def predict(self, data_X, comb=None, data_y=None, return_metric=False):\n",
    "        # prediction = np.zeros(shape = (data_X.shape[0],))\n",
    "        if return_metric :\n",
    "            combs_res = []\n",
    "            self.comb_to_res = {}\n",
    "            num_combs = len(self.combs)\n",
    "            for comb in tqdm(self.combs, desc = \"Progress \", unit = \"Combination\"):\n",
    "#                 print(f\"predict - combs = {self.combs}, comb = {comb}\")\n",
    "                weights = self.get_weights(comb = comb)\n",
    "                prediction = np.zeros(shape = (data_X.shape[0],))\n",
    "                for model_idx in comb :\n",
    "                    if self.is_time_series : \n",
    "                        prediction += self.models_dic[model_idx][\"model\"].predict(data_X).flatten()*weights[model_idx]\n",
    "                    else : \n",
    "                        prediction += self.models_dic[model_idx][\"model\"].predict(data_X)*weights[model_idx]\n",
    "                metric_val = mean_absolute_error(data_y, prediction)\n",
    "                combs_res.append(metric_val)\n",
    "                self.comb_to_res[comb] = metric_val\n",
    "            return combs_res\n",
    "        else:\n",
    "            weights = self.get_weights(comb = comb)\n",
    "            prediction = np.zeros(shape = (data_X.shape[0],))\n",
    "            for model_idx in comb :\n",
    "                if self.is_time_series: \n",
    "                    prediction += self.models_dic[model_idx][\"model\"].predict(data_X).flatten()*weights[model_idx]\n",
    "                else:\n",
    "                    prediction += self.models_dic[model_idx][\"model\"].predict(data_X)*weights[model_idx]\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Function that tests an input model to measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_X, test_y, metric = \"mean_absolute_error\", is_time_series = False):\n",
    "    prediction = model.predict(test_X)\n",
    "    if is_time_series: prediction = prediction.flatten()\n",
    "    metric_val = eval(f\"{metric}(test_y, prediction)\")\n",
    "    return metric_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that tests data batches and find the best batch that if used to train a model will enable the model to get the highest possible score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBatches(data_batch_list, model, data_type = \"normal\", use_Combs = True,\n",
    "                test_on_self = False, return_batches = False, return_models = False,\n",
    "                test_size = 0.3, is_time_series = False):\n",
    "    if is_time_series:\n",
    "        checkpoint_name = './Weights/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "        checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 0, save_best_only = True, mode ='auto')\n",
    "        callbacks_list = [checkpoint]\n",
    "    \n",
    "    if not test_on_self:\n",
    "        # Prepare Test Data (the last batch in the list of batches ):\n",
    "        # Create a Batch object for the test_data_batch:\n",
    "        batch_test = Batch(name = f\"Batch - Test\", df = data_batch_list[-1].copy(), test_size = 0.98)\n",
    "        batch_test.process_data(data_type = data_type)\n",
    "        if not is_time_series: test_X = batch_test.test_X\n",
    "        else : test_X = batch_test.ts_test_X\n",
    "        if not is_time_series: test_y = batch_test.test_y\n",
    "        else :test_y = batch_test.ts_test_y\n",
    "    \n",
    "    if return_batches : comb2batch = {}\n",
    "    if return_models : comb2model = {}\n",
    "\n",
    "    # List of combinations\n",
    "    combs = []\n",
    "    # Generate the combinations:\n",
    "    if use_Combs:\n",
    "        for n in range(1,len(data_batch_list),1):\n",
    "            combs += list(itertools.combinations(range(len(data_batch_list)-1), n))\n",
    "    else : \n",
    "        combs = [tuple([i]) for i in range(len(data_batch_list)-1)] \n",
    "    results_dic = {} \n",
    "    # For every Combination ..\n",
    "    for c,comb in enumerate(tqdm(combs)) :\n",
    "        # Make a compy of the model :\n",
    "        temp_model = copy.deepcopy(model)\n",
    "        # Merge all the batches of the current combination :\n",
    "        temp_data = None\n",
    "        temp_data = data_batch_list[comb[0]].copy()\n",
    "        if len(comb)>1:\n",
    "            for i in range(1,len(comb),1):\n",
    "                temp_data = temp_data.append(data_batch_list[comb[i]].copy(), ignore_index = True)\n",
    "            \n",
    "        # Create a Batch object:\n",
    "        batch_name = f\"Batch - {comb}\"\n",
    "        batch = Batch(name = batch_name, df = temp_data, test_size = test_size)\n",
    "        # Process The Data :\n",
    "        batch.process_data(data_type = data_type)\n",
    "        # Fit the model the processed data :\n",
    "        if not is_time_series : \n",
    "            temp_model.fit(batch.train_X, batch.train_y)\n",
    "        else : \n",
    "            model.fit(batch.ts_train_X, batch.ts_train_y, epochs = 512, batch_size = 32,\n",
    "                      verbose = 0, validation_split = 0.3, callbacks=callbacks_list)\n",
    "        # Measure the goodness of the model :\n",
    "        if test_on_self:\n",
    "            if not is_time_series:\n",
    "                result = test_model(model = temp_model, test_X = batch.test_X, test_y = batch.test_y,\n",
    "                                    metric = \"mean_absolute_error\", is_time_series = is_time_series)\n",
    "            else :\n",
    "                result = test_model(model = temp_model, test_X = batch.ts_test_X, test_y = batch.ts_test_y,\n",
    "                                    metric = \"mean_absolute_error\", is_time_series = is_time_series)\n",
    "        else : \n",
    "            if not is_time_series:\n",
    "                result = test_model(model = temp_model, test_X = test_X, test_y = test_y,\n",
    "                                    metric = \"mean_absolute_error\", is_time_series = is_time_series)\n",
    "            else:\n",
    "                result = test_model(model = temp_model, test_X = test_X, test_y = test_y,\n",
    "                                    metric = \"mean_absolute_error\", is_time_series = is_time_series)\n",
    "        # Add the batch name and its result to the results dictionary:\n",
    "        results_dic[batch_name] = result\n",
    "        if return_batches : comb2batch [batch_name] = copy.deepcopy(batch)\n",
    "        if return_models : comb2model [batch_name] = (copy.deepcopy(temp_model),result)\n",
    "#         print(f\"Done With Comb {c+1} / {len(combs)}\")\n",
    "    \n",
    "    # Convert the dictionary into a DataFrame :\n",
    "    results_df = pd.DataFrame(results_dic.items(), columns=['Batch Name', 'Result'])\n",
    "    results_df = results_df.sort_values(by = \"Result\")\n",
    "    \n",
    "    if return_batches : comb2batch [(-1,)] = copy.deepcopy(batch_test)\n",
    "    \n",
    "    # To Do : return the trained models\n",
    "    if return_batches and return_models :\n",
    "        return results_df, comb2batch, comb2model\n",
    "    elif return_batches and not return_models :\n",
    "        return results_df, comb2batch\n",
    "    elif not return_batches and return_models :\n",
    "        return results_df, comb2model\n",
    "    else:\n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset:\n",
    "df = pd.read_csv(\"Data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = df.shape[0] # Total number of samples\n",
    "# n_batch = (n_samples//1000)\n",
    "n_batch = 5\n",
    "SPB = n_samples//n_batch # samples_per_batch\n",
    "batch_list = []\n",
    "for i in range(n_batch):\n",
    "    batch_list.append(df[i*SPB : i*SPB + SPB].reset_index(drop=True))\n",
    "# batch_list.append(None) # Uncomment when using test_on_self=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Used Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(gamma = \"scale\", kernel = \"rbf\", degree = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batch Grid Search:\n",
    "Finding the best combination of data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8e532f9c824b0cbbbc5c78bae0c4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, comb2batch, comb2model = testBatches(data_batch_list = batch_list,\n",
    "                                        model = model, data_type = \"normal\",\n",
    "                                        use_Combs = True, return_batches = True,\n",
    "                                        return_models = True, test_on_self = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb_name = results_df.reset_index()[\"Batch Name\"][0]\n",
    "best_batch = comb2batch[best_comb_name]\n",
    "test_batch = comb2batch[(-1,)]\n",
    "n_best_models = 5\n",
    "best_models = [model for _, (model, _) in sorted(comb2model.items(), key=lambda item: item[1][1])]\n",
    "best_models = best_models[:min(n_best_models, len(best_models))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the class WeightedAverage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models from the grid search:\n",
    "models_dic = [{\"name\":f\"m{i}\", \"model\":m} for i,m in enumerate(best_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = WeightedAverage(models_dic,is_time_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.calculate_metric(test_batch.test_X, test_batch.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b59d8e3fc3402f8f7a5d7a0e380876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress ', max=31.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time = 0 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wa.find_best_combination(data_X = test_batch.test_X, data_y = test_batch.test_y)\n",
    "end = time.time()\n",
    "print(f\"Time = {int(end-start)//60 } min\")\n",
    "duration = 1  # seconds\n",
    "freq = 350  # Hz\n",
    "_ = os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Combination is ((0,)) with score of (13.335218961648316)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Combination is ({wa.best_combination}) with score of ({wa.comb_to_res[wa.best_combination]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Time Series Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4\n",
    "CNN_Model = CNN_Model(input_shape = (n_steps,1), conv_layers = [128,128], dense_layers = [64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batch Grid Search:\n",
    "Finding the best combination of data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6add984636f45e6bd13e132b96d631a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, comb2batch, comb2model = testBatches(data_batch_list = batch_list,\n",
    "                                        model = CNN_Model, data_type = \"time_series\",\n",
    "                                        use_Combs = True, return_batches = True,\n",
    "                                        return_models = True, test_on_self = False,\n",
    "                                        is_time_series = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb_name = results_df.reset_index()[\"Batch Name\"][0]\n",
    "best_batch = comb2batch[best_comb_name]\n",
    "test_batch = comb2batch[(-1,)]\n",
    "n_best_models = 5\n",
    "best_models = [model for _, (model, _) in sorted(comb2model.items(), key=lambda item: item[1][1])]\n",
    "best_models = best_models[:min(n_best_models, len(best_models))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the class WeightedAverage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models from the grid search:\n",
    "models_dic = [{\"name\":f\"m{i}\", \"model\":m} for i,m in enumerate(best_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = WeightedAverage(models_dic,is_time_series=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.calculate_metric(test_batch.ts_test_X, test_batch.ts_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b235b1e1db4077bdc16b104e5c1981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress ', max=31.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time = 0 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wa.find_best_combination(data_X = test_batch.ts_test_X, data_y = test_batch.ts_test_y)\n",
    "end = time.time()\n",
    "print(f\"Time = {int(end-start)//60 } min\")\n",
    "duration = 1  # seconds\n",
    "freq = 350  # Hz\n",
    "_ = os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Combination is ((0, 1, 2)) with score of (13.117593238450981)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Combination is ({wa.best_combination}) with score of ({wa.comb_to_res[wa.best_combination]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
